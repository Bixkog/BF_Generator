{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "token = {\n",
    "    '\\0' : 0,\n",
    "    '.' : 1, \n",
    "    ',' : 2, \n",
    "    '[' : 3, \n",
    "    ']' : 4, \n",
    "    '<' : 5, \n",
    "    '>' : 6, \n",
    "    '+' : 7, \n",
    "    '-' : 8,\n",
    "    \"START\" : 9\n",
    "    }\n",
    "\n",
    "char = {\n",
    "    0 : '\\0',\n",
    "    1 : '.',\n",
    "    2 : ',', \n",
    "    3 : '[', \n",
    "    4 : ']',  \n",
    "    5 : '<',  \n",
    "    6 : '>', \n",
    "    7 : '+',  \n",
    "    8 : '-'\n",
    "    # no START on purpose\n",
    "    }\n",
    "\n",
    "class BFgen(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, output_size, n_layers=2, batch_size=1):\n",
    "        super(BFgen, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.functional.log_softmax\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    forward\n",
    "    Takes input_token and hidden memory state <- input to recursive layer\n",
    "    returns output token and changed hidden memory state.\n",
    "    \"\"\"\n",
    "    def forward(self, input_token, hidden):\n",
    "        embeds = self.encoder(input_token)\n",
    "        output, hidden = self.lstm(\n",
    "            embeds.view(len(input_token), self.batch_size, -1), hidden)\n",
    "        output = self.decoder(output.view(self.batch_size, -1))\n",
    "        output = self.softmax(output) # in paper its multinomial distribution\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden_zero(self):\n",
    "        self.hidden = (Variable(torch.zeros(self.n_layers, self.batch_size, self.hidden_size)),\n",
    "                      Variable(torch.zeros(self.n_layers, self.batch_size, self.hidden_size)),)\n",
    "    \n",
    "    def init_hidden_normal(self):\n",
    "        means = torch.zeros(self.n_layers, self.batch_size, self.hidden_size)\n",
    "        std = torch.Tensor([0.001]*self.hidden_size*self.n_layers*self.batch_size).unsqueeze(0)\n",
    "        self.hidden = (Variable(torch.normal(means, std)), Variable(torch.normal(means, std)))\n",
    "\n",
    "    def evaluate(self, predict_len=100):\n",
    "        input_token = token[\"START\"]\n",
    "        hidden = self.init_hidden_zero\n",
    "        prediction = \"\"\n",
    "\n",
    "        for i in range(predict_len):\n",
    "            output_token, hidden = self.forward(input_token, hidden)\n",
    "            input_token = output_token\n",
    "\n",
    "            prediction += char[output_token]\n",
    "            if output_token == '\\0':\n",
    "                break\n",
    "\n",
    "        return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 10\n",
    "hidden_size = 35\n",
    "output_size = 9\n",
    "n_layers = 2\n",
    "\n",
    "token_num = len(token.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BFgen(10, embedding_size, hidden_size, output_size, n_layers, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_hidden_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      "1.00000e-03 *\n",
      " -1.5843  0.1523  0.7197  ...   1.1720 -1.2670 -0.7931\n",
      " -0.7721 -0.0372  0.7097  ...   0.8015  0.6435 -0.1788\n",
      "  0.9605  0.5317  1.0641  ...   0.7549  0.5456  0.4341\n",
      "           ...             ⋱             ...          \n",
      "  0.2711  0.2142 -0.5712  ...   0.3432  0.6767 -0.3111\n",
      "  0.1518 -0.3111 -1.1244  ...   0.9975  1.4528  0.3571\n",
      " -0.4659  1.0484  0.4992  ...  -1.2222 -0.3975 -0.7004\n",
      "\n",
      "(1 ,.,.) = \n",
      "1.00000e-03 *\n",
      "  0.0843 -0.6299 -0.3144  ...  -0.1246  1.3298  0.6861\n",
      "  0.0007 -1.5905 -0.3623  ...   0.2020 -2.0490 -1.2851\n",
      " -0.2309  0.7680 -0.1753  ...  -1.6299  0.8580 -2.0703\n",
      "           ...             ⋱             ...          \n",
      "  2.3861  0.3732 -0.0975  ...   1.1056 -0.6599 -0.2478\n",
      "  1.0782  0.1525 -0.6702  ...   0.6983  0.2251 -1.2922\n",
      "  0.4027 -1.1268 -2.6957  ...   0.7005  1.3965  0.4474\n",
      "[torch.FloatTensor of size 2x64x35]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      "1.00000e-03 *\n",
      "  1.9834 -0.8784 -1.4083  ...  -1.7819 -0.9550 -1.4679\n",
      " -0.4587 -0.3098  1.2490  ...   1.8168 -0.1055  0.1303\n",
      "  0.6894  0.4703  0.8999  ...   0.7055 -0.0263 -1.2660\n",
      "           ...             ⋱             ...          \n",
      " -1.1505  0.0060 -0.0621  ...   0.1067 -1.0001 -1.4180\n",
      "  0.3050 -0.3212  0.9413  ...   0.8243  0.0051  0.8803\n",
      " -1.0213 -1.3745 -0.2729  ...  -0.7096 -0.7506  0.8199\n",
      "\n",
      "(1 ,.,.) = \n",
      "1.00000e-03 *\n",
      " -1.2654 -1.1621  0.9010  ...  -2.7653 -0.8447 -0.0364\n",
      "  0.7788  0.3661 -1.1671  ...   0.1156 -0.1776  0.8326\n",
      "  0.1699 -1.2609  0.6086  ...   0.1730 -0.1616  0.5274\n",
      "           ...             ⋱             ...          \n",
      "  1.0908 -0.8864 -0.2446  ...  -0.2923  1.5268  0.0183\n",
      "  0.8653  1.1577  0.0611  ...  -0.0118  0.2976  0.1406\n",
      "  1.8895  1.0691 -0.4101  ...  -1.4779  0.0056  0.1032\n",
      "[torch.FloatTensor of size 2x64x35]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_tensor(input_token):\n",
    "    tensor = torch.zeros(1, token_num).long()\n",
    "    tensor[0][token[input_token]] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "[torch.LongTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_sample = token_to_tensor(\">\")\n",
    "print input_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: size '[1 x 64 x -1]' is invalid for input with 100 elements at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/THStorage.c:37",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3e7f35cc40d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-2b313ba02777>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_token, hidden)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         output, hidden = self.lstm(\n\u001b[0;32m---> 56\u001b[0;31m             embeds.view(len(input_token), self.batch_size, -1), hidden)\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# in paper its multinomial distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: size '[1 x 64 x -1]' is invalid for input with 100 elements at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/THStorage.c:37"
     ]
    }
   ],
   "source": [
    "model.forward(Variable(input_sample), model.hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
