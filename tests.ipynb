{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "token = {\n",
    "    '.' : 0, \n",
    "    ',' : 1, \n",
    "    '[' : 2, \n",
    "    ']' : 3, \n",
    "    '<' : 4, \n",
    "    '>' : 5, \n",
    "    '+' : 6, \n",
    "    '-' : 7,\n",
    "    \"START\" : 8\n",
    "    }\n",
    "\n",
    "char = {\n",
    "    0 : '.',\n",
    "    1 : ',', \n",
    "    2 : '[', \n",
    "    3 : ']',  \n",
    "    4 : '<',  \n",
    "    5 : '>', \n",
    "    6 : '+',  \n",
    "    7 : '-'\n",
    "    # no START on purpose\n",
    "    }\n",
    "\n",
    "class BFgen(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, output_size, n_layers=2, batch_size=1):\n",
    "        super(BFgen, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.functional.softmax\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    forward\n",
    "    Takes input_token and hidden memory state <- input to recursive layer\n",
    "    returns output token and changed hidden memory state.\n",
    "    \"\"\"\n",
    "    def forward(self, input_token):\n",
    "        embeds = self.encoder(input_token)\n",
    "        output, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        decoded = self.decoder(output)\n",
    "        probs = self.softmax(decoded[-1])\n",
    "        return probs\n",
    "    \n",
    "    def init_hidden_zero(self):\n",
    "        self.hidden = (Variable(torch.zeros(self.n_layers, self.batch_size, self.hidden_size)),\n",
    "                      Variable(torch.zeros(self.n_layers, self.batch_size, self.hidden_size)),)\n",
    "    \n",
    "    def init_hidden_normal(self, variance=0.01):\n",
    "        means = torch.zeros(self.n_layers, self.batch_size, self.hidden_size)\n",
    "        std = torch.Tensor([variance]*self.hidden_size*self.n_layers*self.batch_size).unsqueeze(0)\n",
    "        self.hidden = (Variable(torch.normal(means, std)), Variable(torch.normal(means, std)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_tensor(input_token):\n",
    "    tensor = torch.zeros(1, token_num).long()\n",
    "    tensor[0][token[input_token]] = 1\n",
    "    return tensor\n",
    "\n",
    "def pred(sample):\n",
    "    return np.argsort(sample)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 10\n",
    "hidden_size = 35\n",
    "output_size = len(char.keys())\n",
    "n_layers = 2\n",
    "batch_size = 16\n",
    "\n",
    "token_num = len(token.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, predict_len=100, variance=0.01):\n",
    "    input_token = \"START\"\n",
    "    input_token = token_to_tensor(input_token)\n",
    "    model.init_hidden_normal(variance=0.5)\n",
    "    prediction = [\"\"] * batch_size\n",
    "    program_probs = np.ones((1, batch_size))\n",
    "    \n",
    "    batched_input = torch.zeros((batch_size, token_num)).long()\n",
    "    batched_input = batched_input + input_token\n",
    "    batched_input = Variable(batched_input.view(token_num, batch_size))\n",
    "    \n",
    "    for i in range(predict_len):\n",
    "        output_probs = model.forward(batched_input)\n",
    "        next_tokens = np.apply_along_axis(pred, axis = 1, arr=output_probs.data.numpy())\n",
    "        top_probs = output_probs[np.arange(batch_size), next_tokens]\n",
    "        \n",
    "        batched_input = torch.zeros((batch_size, token_num)).long()\n",
    "        batched_input[np.arange(batch_size), next_tokens] = 1\n",
    "        batched_input = Variable(batched_input.view(token_num, batch_size))\n",
    "        \n",
    "        program_probs *= top_probs.data.numpy()\n",
    "        \n",
    "        #prediction = map(\"\".join,zip(prediction, char[]))\n",
    "        for i in xrange(batch_size):\n",
    "            prediction[i] += char[next_tokens[i]]\n",
    "        \n",
    "    return prediction, program_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BFgen(token_num, embedding_size, hidden_size, output_size, n_layers, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Wojtek/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "progs, probs = evaluate(model, 5, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['..[[.', '...[[', '[....', '..[[[', '[....', '.[[[[', '[....', '.[[[[', '.....', '.[...', '.....', '[....', '.[.[.', '[....', '.[[.[', '[....']\n"
     ]
    }
   ],
   "source": [
    "print(progs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
