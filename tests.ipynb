{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "token = {\n",
    "    '\\0' : 0,\n",
    "    '.' : 1, \n",
    "    ',' : 2, \n",
    "    '[' : 3, \n",
    "    ']' : 4, \n",
    "    '<' : 5, \n",
    "    '>' : 6, \n",
    "    '+' : 7, \n",
    "    '-' : 8,\n",
    "    \"START\" : 9\n",
    "    }\n",
    "\n",
    "char = {\n",
    "    0 : '\\0',\n",
    "    1 : '.',\n",
    "    2 : ',', \n",
    "    3 : '[', \n",
    "    4 : ']',  \n",
    "    5 : '<',  \n",
    "    6 : '>', \n",
    "    7 : '+',  \n",
    "    8 : '-'\n",
    "    # no START on purpose\n",
    "    }\n",
    "\n",
    "class BFgen(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, output_size, n_layers=2, batch_size=1):\n",
    "        super(BFgen, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, n_layers, batch_first=True)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.functional.softmax\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    forward\n",
    "    Takes input_token and hidden memory state <- input to recursive layer\n",
    "    returns output token and changed hidden memory state.\n",
    "    \"\"\"\n",
    "    def forward(self, input_token, hidden):\n",
    "        embeds = self.encoder(input_token)\n",
    "        output, hidden = self.lstm(embeds, hidden)\n",
    "        output = self.decoder(output.view(self.batch_size, -1))\n",
    "        output = self.softmax(output) # in paper its multinomial distribution\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden_zero(self):\n",
    "        self.hidden = (Variable(torch.zeros(self.n_layers, self.batch_size, self.hidden_size)),\n",
    "                      Variable(torch.zeros(self.n_layers, self.batch_size, self.hidden_size)),)\n",
    "    \n",
    "    def init_hidden_normal(self):\n",
    "        means = torch.zeros(self.n_layers, self.batch_size, self.hidden_size)\n",
    "        std = torch.Tensor([0.001]*self.hidden_size*self.n_layers*self.batch_size).unsqueeze(0)\n",
    "        self.hidden = (Variable(torch.normal(means, std)), Variable(torch.normal(means, std)))\n",
    "\n",
    "    def evaluate(self, predict_len=100):\n",
    "        input_token = token[\"START\"]\n",
    "        hidden = self.init_hidden_zero\n",
    "        prediction = \"\"\n",
    "\n",
    "        for i in range(predict_len):\n",
    "            output_token, hidden = self.forward(input_token, hidden)\n",
    "            input_token = output_token\n",
    "\n",
    "            prediction += char[output_token]\n",
    "            if output_token == '\\0':\n",
    "                break\n",
    "\n",
    "        return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 10\n",
    "hidden_size = 35\n",
    "output_size = 9\n",
    "n_layers = 2\n",
    "\n",
    "token_num = len(token.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = 64\n",
    "model = BFgen(10, embedding_size, hidden_size, output_size, n_layers, batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_hidden_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      " -1.4069e-03 -1.4254e-04  1.2972e-03  ...   9.3532e-04 -6.7219e-04  1.7171e-05\n",
      "  1.5643e-03 -2.7541e-04 -3.2768e-04  ...  -1.7917e-03 -2.1055e-04 -6.8826e-04\n",
      " -1.0524e-03 -8.5756e-04 -1.3397e-03  ...   9.3815e-04 -9.7098e-04  1.3971e-03\n",
      "                 ...                   ⋱                   ...                \n",
      "  7.1501e-05  6.9305e-04  1.3175e-03  ...  -2.6004e-04  2.2829e-05  1.0682e-03\n",
      "  4.2999e-04 -1.1089e-03  5.8171e-05  ...  -1.5347e-03  5.3940e-04 -1.1950e-03\n",
      "  8.6621e-04  2.3354e-03  1.4715e-03  ...  -5.4765e-04  4.1961e-04 -2.3232e-04\n",
      "\n",
      "(1 ,.,.) = \n",
      " -1.0675e-03  3.6281e-04 -8.9925e-04  ...  -1.9488e-03  1.3730e-04 -1.3119e-03\n",
      "  4.4776e-04  1.0271e-03 -2.0652e-03  ...  -1.0729e-04  2.7098e-04 -8.4759e-05\n",
      "  4.4063e-05  4.1087e-05 -1.5224e-03  ...  -1.5179e-03  1.8758e-03 -6.6197e-04\n",
      "                 ...                   ⋱                   ...                \n",
      "  3.5258e-05  1.4627e-03  1.5303e-03  ...  -1.2200e-03 -8.2137e-04 -4.4383e-04\n",
      " -7.5204e-04  1.0114e-03  4.8033e-04  ...  -7.1905e-06  1.7001e-03 -7.8663e-05\n",
      "  8.5535e-05  3.0179e-03 -7.7081e-04  ...   3.3451e-04  2.1754e-04 -8.7948e-04\n",
      "[torch.FloatTensor of size 2x64x35]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      "1.00000e-03 *\n",
      "  0.5714  0.0289 -1.4895  ...  -0.1358  0.8259  1.6042\n",
      " -0.9266 -0.9761  0.8417  ...   1.0544  0.4651  0.0448\n",
      "  1.1644 -0.4433 -0.5149  ...   1.5265 -1.3105  0.5959\n",
      "           ...             ⋱             ...          \n",
      "  0.6037 -0.6522 -0.1423  ...  -0.0744  0.6516  0.5207\n",
      " -0.4054 -0.0566 -1.0615  ...  -1.0028  1.7799 -2.2065\n",
      " -1.0780  1.1084 -0.3004  ...  -0.1849 -0.9259 -0.8895\n",
      "\n",
      "(1 ,.,.) = \n",
      "1.00000e-03 *\n",
      " -0.4212  0.8524 -0.8985  ...   1.2122 -0.9632 -0.4580\n",
      "  0.2333 -0.1407  0.8390  ...  -0.2335 -1.1267  0.7249\n",
      "  0.9782  0.8075  1.0207  ...  -0.4157  1.3913  2.2478\n",
      "           ...             ⋱             ...          \n",
      "  0.1889  1.1705  1.7665  ...   0.8894  2.2174  0.6586\n",
      "  1.7045 -0.1957 -1.0791  ...   0.1525  0.1965 -1.2608\n",
      "  0.4657 -0.9409  1.7269  ...   0.5882 -0.3737 -0.3069\n",
      "[torch.FloatTensor of size 2x64x35]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_tensor(input_token):\n",
    "    tensor = torch.zeros(1, token_num).long()\n",
    "    tensor[0][token[input_token]] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0\n",
      "[torch.LongTensor of size 1x64x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_sample = token_to_tensor(\">\")\n",
    "batched_input = torch.zeros((1, 64, token_num)).long()\n",
    "batched_input = batched_input + input_sample\n",
    "\n",
    "batch_input_sizes = [token_num] * batch_num\n",
    "print batched_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batched_input = Variable(batched_input.view(batch_num, -1))#= nn.utils.rnn.pack_padded_sequence(Variable(batched_input), batch_input_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "[torch.LongTensor of size 64x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print batched_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(token_num, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      "           ...             ⋱             ...          \n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      "           ...             ⋱             ...          \n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      "\n",
      "(2 ,.,.) = \n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      "           ...             ⋱             ...          \n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      "...\n",
      "\n",
      "(61,.,.) = \n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      "           ...             ⋱             ...          \n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      "\n",
      "(62,.,.) = \n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      "           ...             ⋱             ...          \n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      "\n",
      "(63,.,.) = \n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      "           ...             ⋱             ...          \n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      " -0.3275 -1.7213  0.1308  ...  -1.2300 -0.8907 -1.8747\n",
      "[torch.FloatTensor of size 64x10x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeds = emb(batched_input)\n",
    "print embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hid = model.lstm(embeds, model.hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -2.2180e-02  4.2906e-03 -3.3553e-02  ...  -3.0389e-02  1.3468e-02 -3.5761e-02\n",
      " -4.0556e-02  5.2905e-03 -5.5761e-02  ...  -3.6525e-02  1.4040e-02 -6.2871e-02\n",
      " -5.3736e-02  4.8165e-03 -6.9306e-02  ...  -3.6038e-02  9.5348e-03 -8.3626e-02\n",
      "                 ...                   ⋱                   ...                \n",
      " -4.4544e-02 -5.8202e-03 -8.8912e-02  ...  -7.2345e-02  1.1444e-03 -9.7088e-02\n",
      " -5.2288e-02 -4.0085e-03 -8.9549e-02  ...  -6.3323e-02 -8.7049e-04 -1.0026e-01\n",
      " -6.0355e-02 -2.9574e-03 -8.9246e-02  ...  -5.4888e-02 -2.5100e-03 -1.0559e-01\n",
      "\n",
      "(1 ,.,.) = \n",
      " -2.1838e-02  4.2149e-03 -3.3200e-02  ...  -3.1019e-02  1.3527e-02 -3.5435e-02\n",
      " -4.0280e-02  5.2634e-03 -5.5643e-02  ...  -3.6972e-02  1.4114e-02 -6.2785e-02\n",
      " -5.3522e-02  4.8131e-03 -6.9254e-02  ...  -3.6359e-02  9.5688e-03 -8.3659e-02\n",
      "                 ...                   ⋱                   ...                \n",
      " -4.4503e-02 -5.8148e-03 -8.8894e-02  ...  -7.2393e-02  1.1349e-03 -9.7137e-02\n",
      " -5.2261e-02 -4.0040e-03 -8.9535e-02  ...  -6.3354e-02 -8.7738e-04 -1.0030e-01\n",
      " -6.0337e-02 -2.9537e-03 -8.9234e-02  ...  -5.4909e-02 -2.5148e-03 -1.0562e-01\n",
      "\n",
      "(2 ,.,.) = \n",
      " -2.1589e-02  4.5346e-03 -3.3156e-02  ...  -3.1156e-02  1.4114e-02 -3.5247e-02\n",
      " -4.0134e-02  5.3973e-03 -5.5651e-02  ...  -3.6983e-02  1.4392e-02 -6.2589e-02\n",
      " -5.3437e-02  4.8761e-03 -6.9287e-02  ...  -3.6311e-02  9.7069e-03 -8.3487e-02\n",
      "                 ...                   ⋱                   ...                \n",
      " -4.4503e-02 -5.8104e-03 -8.8905e-02  ...  -7.2368e-02  1.1438e-03 -9.7099e-02\n",
      " -5.2263e-02 -4.0021e-03 -8.9543e-02  ...  -6.3337e-02 -8.7170e-04 -1.0027e-01\n",
      " -6.0339e-02 -2.9533e-03 -8.9240e-02  ...  -5.4897e-02 -2.5111e-03 -1.0560e-01\n",
      "...\n",
      "\n",
      "(61,.,.) = \n",
      " -2.1758e-02  4.2193e-03 -3.2942e-02  ...  -3.0524e-02  1.4426e-02 -3.5596e-02\n",
      " -4.0311e-02  5.2110e-03 -5.5430e-02  ...  -3.6547e-02  1.4656e-02 -6.2837e-02\n",
      " -5.3577e-02  4.7656e-03 -6.9128e-02  ...  -3.6039e-02  9.8987e-03 -8.3646e-02\n",
      "                 ...                   ⋱                   ...                \n",
      " -4.4514e-02 -5.8191e-03 -8.8901e-02  ...  -7.2348e-02  1.1582e-03 -9.7105e-02\n",
      " -5.2270e-02 -4.0074e-03 -8.9542e-02  ...  -6.3325e-02 -8.6380e-04 -1.0027e-01\n",
      " -6.0344e-02 -2.9566e-03 -8.9241e-02  ...  -5.4890e-02 -2.5068e-03 -1.0560e-01\n",
      "\n",
      "(62,.,.) = \n",
      " -2.1628e-02  3.9919e-03 -3.3701e-02  ...  -3.0779e-02  1.4045e-02 -3.5958e-02\n",
      " -4.0219e-02  5.1329e-03 -5.5861e-02  ...  -3.6794e-02  1.4370e-02 -6.3089e-02\n",
      " -5.3526e-02  4.7413e-03 -6.9354e-02  ...  -3.6238e-02  9.7067e-03 -8.3804e-02\n",
      "                 ...                   ⋱                   ...                \n",
      " -4.4516e-02 -5.8184e-03 -8.8901e-02  ...  -7.2378e-02  1.1497e-03 -9.7116e-02\n",
      " -5.2270e-02 -4.0063e-03 -8.9539e-02  ...  -6.3345e-02 -8.6716e-04 -1.0028e-01\n",
      " -6.0343e-02 -2.9552e-03 -8.9238e-02  ...  -5.4903e-02 -2.5076e-03 -1.0561e-01\n",
      "\n",
      "(63,.,.) = \n",
      " -2.1597e-02  3.9458e-03 -3.3021e-02  ...  -3.0700e-02  1.3402e-02 -3.5510e-02\n",
      " -4.0148e-02  5.0730e-03 -5.5493e-02  ...  -3.6696e-02  1.4074e-02 -6.2867e-02\n",
      " -5.3458e-02  4.6914e-03 -6.9163e-02  ...  -3.6146e-02  9.5779e-03 -8.3720e-02\n",
      "                 ...                   ⋱                   ...                \n",
      " -4.4498e-02 -5.8189e-03 -8.8899e-02  ...  -7.2357e-02  1.1443e-03 -9.7142e-02\n",
      " -5.2257e-02 -4.0053e-03 -8.9540e-02  ...  -6.3330e-02 -8.7158e-04 -1.0030e-01\n",
      " -6.0334e-02 -2.9537e-03 -8.9240e-02  ...  -5.4893e-02 -2.5115e-03 -1.0562e-01\n",
      "[torch.FloatTensor of size 64x10x35]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.decoder(out).view(10,64,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.0744  0.0885  0.1237  ...   0.0908  0.1713 -0.1305\n",
      "  0.1045  0.1041  0.1243  ...   0.0788  0.1893 -0.1247\n",
      "  0.1233  0.1156  0.1247  ...   0.0700  0.2022 -0.1202\n",
      "           ...             ⋱             ...          \n",
      "  0.1042  0.1041  0.1243  ...   0.0789  0.1893 -0.1249\n",
      "  0.1232  0.1155  0.1247  ...   0.0701  0.2021 -0.1204\n",
      "  0.1350  0.1236  0.1248  ...   0.0645  0.2109 -0.1176\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.1425  0.1293  0.1247  ...   0.0611  0.2167 -0.1159\n",
      "  0.1471  0.1334  0.1246  ...   0.0591  0.2205 -0.1150\n",
      "  0.1400  0.1233  0.1196  ...   0.0732  0.1999 -0.1404\n",
      "           ...             ⋱             ...          \n",
      "  0.1471  0.1333  0.1246  ...   0.0591  0.2205 -0.1150\n",
      "  0.1400  0.1233  0.1196  ...   0.0732  0.1999 -0.1404\n",
      "  0.1379  0.1241  0.1202  ...   0.0712  0.2030 -0.1415\n",
      "\n",
      "(2 ,.,.) = \n",
      "  0.1400  0.1264  0.1209  ...   0.0678  0.2071 -0.1378\n",
      "  0.1430  0.1291  0.1217  ...   0.0645  0.2112 -0.1325\n",
      "  0.0743  0.0883  0.1238  ...   0.0908  0.1710 -0.1310\n",
      "           ...             ⋱             ...          \n",
      "  0.1430  0.1291  0.1217  ...   0.0645  0.2112 -0.1325\n",
      "  0.0740  0.0881  0.1236  ...   0.0908  0.1710 -0.1309\n",
      "  0.1042  0.1040  0.1243  ...   0.0788  0.1892 -0.1249\n",
      "...\n",
      "\n",
      "(7 ,.,.) = \n",
      "  0.1400  0.1264  0.1209  ...   0.0678  0.2071 -0.1378\n",
      "  0.1430  0.1291  0.1217  ...   0.0645  0.2112 -0.1325\n",
      "  0.0740  0.0886  0.1238  ...   0.0909  0.1712 -0.1307\n",
      "           ...             ⋱             ...          \n",
      "  0.1430  0.1291  0.1217  ...   0.0645  0.2112 -0.1325\n",
      "  0.0738  0.0881  0.1236  ...   0.0909  0.1707 -0.1308\n",
      "  0.1041  0.1039  0.1243  ...   0.0788  0.1891 -0.1249\n",
      "\n",
      "(8 ,.,.) = \n",
      "  0.1231  0.1155  0.1247  ...   0.0701  0.2021 -0.1203\n",
      "  0.1350  0.1236  0.1248  ...   0.0645  0.2108 -0.1175\n",
      "  0.1425  0.1293  0.1247  ...   0.0611  0.2167 -0.1159\n",
      "           ...             ⋱             ...          \n",
      "  0.1351  0.1236  0.1248  ...   0.0645  0.2108 -0.1176\n",
      "  0.1425  0.1293  0.1247  ...   0.0611  0.2167 -0.1159\n",
      "  0.1472  0.1333  0.1246  ...   0.0591  0.2205 -0.1150\n",
      "\n",
      "(9 ,.,.) = \n",
      "  0.1400  0.1233  0.1196  ...   0.0732  0.1999 -0.1404\n",
      "  0.1379  0.1241  0.1202  ...   0.0712  0.2030 -0.1415\n",
      "  0.1400  0.1264  0.1209  ...   0.0677  0.2071 -0.1378\n",
      "           ...             ⋱             ...          \n",
      "  0.1379  0.1241  0.1202  ...   0.0712  0.2030 -0.1415\n",
      "  0.1400  0.1264  0.1209  ...   0.0678  0.2071 -0.1378\n",
      "  0.1430  0.1291  0.1217  ...   0.0645  0.2112 -0.1325\n",
      "[torch.FloatTensor of size 10x64x9]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.1175  0.1156  0.1151  0.1207  0.0947  0.1129  0.1099  0.1248  0.0888\n",
      " 0.1172  0.1156  0.1152  0.1210  0.0949  0.1127  0.1097  0.1251  0.0887\n",
      " 0.1173  0.1157  0.1151  0.1210  0.0948  0.1125  0.1091  0.1255  0.0889\n",
      " 0.1175  0.1159  0.1150  0.1209  0.0946  0.1125  0.1086  0.1258  0.0892\n",
      " 0.1115  0.1130  0.1172  0.1210  0.0990  0.1115  0.1133  0.1228  0.0908\n",
      " 0.1140  0.1139  0.1163  0.1209  0.0971  0.1121  0.1111  0.1240  0.0906\n",
      " 0.1156  0.1147  0.1157  0.1206  0.0958  0.1124  0.1096  0.1250  0.0906\n",
      " 0.1166  0.1152  0.1154  0.1204  0.0951  0.1125  0.1086  0.1257  0.0905\n",
      " 0.1172  0.1156  0.1151  0.1202  0.0946  0.1125  0.1080  0.1262  0.0905\n",
      " 0.1176  0.1160  0.1149  0.1201  0.0943  0.1125  0.1077  0.1265  0.0905\n",
      " 0.1175  0.1156  0.1151  0.1207  0.0947  0.1129  0.1099  0.1248  0.0888\n",
      " 0.1172  0.1156  0.1152  0.1210  0.0949  0.1126  0.1097  0.1251  0.0886\n",
      " 0.1173  0.1157  0.1151  0.1210  0.0948  0.1125  0.1092  0.1255  0.0889\n",
      " 0.1175  0.1159  0.1150  0.1209  0.0946  0.1125  0.1086  0.1258  0.0892\n",
      " 0.1114  0.1130  0.1171  0.1210  0.0990  0.1114  0.1133  0.1228  0.0908\n",
      " 0.1140  0.1139  0.1163  0.1209  0.0971  0.1121  0.1111  0.1241  0.0906\n",
      " 0.1155  0.1147  0.1157  0.1206  0.0959  0.1124  0.1096  0.1250  0.0906\n",
      " 0.1166  0.1152  0.1154  0.1204  0.0951  0.1125  0.1086  0.1257  0.0905\n",
      " 0.1172  0.1156  0.1151  0.1202  0.0946  0.1125  0.1080  0.1262  0.0905\n",
      " 0.1176  0.1160  0.1149  0.1201  0.0943  0.1125  0.1077  0.1265  0.0905\n",
      " 0.1175  0.1156  0.1151  0.1207  0.0947  0.1129  0.1099  0.1248  0.0888\n",
      " 0.1172  0.1156  0.1152  0.1210  0.0949  0.1127  0.1097  0.1251  0.0887\n",
      " 0.1173  0.1157  0.1151  0.1210  0.0948  0.1125  0.1091  0.1255  0.0889\n",
      " 0.1175  0.1159  0.1150  0.1209  0.0946  0.1125  0.1086  0.1258  0.0892\n",
      " 0.1115  0.1130  0.1171  0.1210  0.0990  0.1115  0.1133  0.1228  0.0908\n",
      " 0.1140  0.1139  0.1163  0.1209  0.0971  0.1121  0.1111  0.1241  0.0906\n",
      " 0.1155  0.1147  0.1157  0.1206  0.0958  0.1124  0.1096  0.1250  0.0906\n",
      " 0.1166  0.1152  0.1154  0.1204  0.0951  0.1125  0.1086  0.1257  0.0905\n",
      " 0.1172  0.1156  0.1151  0.1202  0.0946  0.1125  0.1080  0.1262  0.0905\n",
      " 0.1176  0.1160  0.1149  0.1201  0.0943  0.1125  0.1077  0.1265  0.0905\n",
      " 0.1175  0.1156  0.1151  0.1207  0.0947  0.1129  0.1099  0.1248  0.0888\n",
      " 0.1172  0.1156  0.1152  0.1210  0.0949  0.1126  0.1097  0.1251  0.0887\n",
      " 0.1173  0.1157  0.1151  0.1210  0.0948  0.1125  0.1091  0.1255  0.0889\n",
      " 0.1175  0.1159  0.1150  0.1209  0.0946  0.1125  0.1086  0.1258  0.0892\n",
      " 0.1115  0.1131  0.1171  0.1210  0.0990  0.1115  0.1133  0.1228  0.0908\n",
      " 0.1140  0.1139  0.1163  0.1209  0.0971  0.1121  0.1111  0.1241  0.0906\n",
      " 0.1155  0.1147  0.1157  0.1206  0.0958  0.1124  0.1096  0.1250  0.0906\n",
      " 0.1166  0.1152  0.1154  0.1204  0.0951  0.1125  0.1086  0.1257  0.0905\n",
      " 0.1172  0.1157  0.1151  0.1202  0.0946  0.1125  0.1080  0.1262  0.0905\n",
      " 0.1176  0.1160  0.1149  0.1201  0.0943  0.1125  0.1077  0.1265  0.0905\n",
      " 0.1175  0.1156  0.1151  0.1207  0.0947  0.1129  0.1099  0.1248  0.0888\n",
      " 0.1172  0.1156  0.1152  0.1210  0.0949  0.1126  0.1097  0.1251  0.0887\n",
      " 0.1173  0.1157  0.1151  0.1210  0.0948  0.1125  0.1091  0.1255  0.0889\n",
      " 0.1175  0.1159  0.1150  0.1209  0.0946  0.1125  0.1086  0.1258  0.0892\n",
      " 0.1114  0.1131  0.1171  0.1210  0.0990  0.1115  0.1133  0.1228  0.0908\n",
      " 0.1140  0.1139  0.1163  0.1209  0.0971  0.1121  0.1111  0.1241  0.0906\n",
      " 0.1155  0.1147  0.1157  0.1206  0.0958  0.1124  0.1096  0.1250  0.0906\n",
      " 0.1166  0.1152  0.1154  0.1204  0.0951  0.1125  0.1086  0.1257  0.0905\n",
      " 0.1172  0.1157  0.1151  0.1202  0.0946  0.1125  0.1080  0.1262  0.0905\n",
      " 0.1176  0.1160  0.1149  0.1201  0.0943  0.1125  0.1077  0.1265  0.0905\n",
      " 0.1175  0.1156  0.1151  0.1207  0.0947  0.1129  0.1099  0.1248  0.0888\n",
      " 0.1172  0.1156  0.1152  0.1210  0.0949  0.1126  0.1097  0.1251  0.0886\n",
      " 0.1173  0.1157  0.1151  0.1210  0.0948  0.1125  0.1092  0.1255  0.0889\n",
      " 0.1175  0.1159  0.1150  0.1209  0.0946  0.1125  0.1086  0.1258  0.0892\n",
      " 0.1115  0.1131  0.1171  0.1210  0.0990  0.1115  0.1134  0.1228  0.0908\n",
      " 0.1140  0.1139  0.1163  0.1209  0.0971  0.1121  0.1111  0.1240  0.0906\n",
      " 0.1156  0.1147  0.1157  0.1206  0.0958  0.1124  0.1096  0.1250  0.0906\n",
      " 0.1166  0.1152  0.1154  0.1204  0.0951  0.1125  0.1086  0.1257  0.0905\n",
      " 0.1172  0.1157  0.1151  0.1202  0.0946  0.1125  0.1080  0.1262  0.0905\n",
      " 0.1176  0.1160  0.1149  0.1201  0.0943  0.1125  0.1077  0.1265  0.0905\n",
      " 0.1175  0.1156  0.1151  0.1207  0.0947  0.1129  0.1099  0.1248  0.0888\n",
      " 0.1172  0.1156  0.1152  0.1210  0.0949  0.1127  0.1097  0.1251  0.0887\n",
      " 0.1173  0.1157  0.1151  0.1210  0.0948  0.1125  0.1091  0.1255  0.0889\n",
      " 0.1175  0.1159  0.1150  0.1209  0.0946  0.1125  0.1086  0.1258  0.0892\n",
      "[torch.FloatTensor of size 64x9]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Wojtek/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "res = model.softmax(out[-1])\n",
    "print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(sample):\n",
    "    return char[np.argsort(sample)[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = res.data[0].numpy()\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'+'"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.apply_along_axis(pred, axis = 0, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
