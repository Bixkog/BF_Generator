{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "token = {\n",
    "    '.' : 0, \n",
    "    ',' : 1, \n",
    "    '[' : 2, \n",
    "    ']' : 3, \n",
    "    '<' : 4, \n",
    "    '>' : 5, \n",
    "    '+' : 6, \n",
    "    '-' : 7,\n",
    "    \"START\" : 8\n",
    "    }\n",
    "\n",
    "char = {\n",
    "    0 : '.',\n",
    "    1 : ',', \n",
    "    2 : '[', \n",
    "    3 : ']',  \n",
    "    4 : '<',  \n",
    "    5 : '>', \n",
    "    6 : '+',  \n",
    "    7 : '-'\n",
    "    # no START on purpose\n",
    "    }\n",
    "\n",
    "class BFgen(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, output_size, n_layers=2, batch_size=1):\n",
    "        super(BFgen, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.functional.softmax\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    forward\n",
    "    Takes input_token and hidden memory state <- input to recursive layer\n",
    "    returns output token and changed hidden memory state.\n",
    "    \"\"\"\n",
    "    def forward(self, input_token):\n",
    "        embeds = self.encoder(input_token)\n",
    "        output, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        decoded = self.decoder(output)\n",
    "        probs = self.softmax(decoded[-1])\n",
    "        return probs\n",
    "    \n",
    "    def init_hidden_zero(self):\n",
    "        self.hidden = (Variable(torch.zeros(self.n_layers, self.batch_size, self.hidden_size)),\n",
    "                      Variable(torch.zeros(self.n_layers, self.batch_size, self.hidden_size)),)\n",
    "    \n",
    "    def init_hidden_normal(self, variance=0.01):\n",
    "        means = torch.zeros(self.n_layers, self.batch_size, self.hidden_size)\n",
    "        std = torch.Tensor([variance]*self.hidden_size*self.n_layers*self.batch_size).unsqueeze(0)\n",
    "        self.hidden = (Variable(torch.normal(means, std)), Variable(torch.normal(means, std)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_tensor(input_token):\n",
    "    tensor = torch.zeros(1, token_num).long()\n",
    "    tensor[0][token[input_token]] = 1\n",
    "    return tensor\n",
    "\n",
    "def pred(sample):\n",
    "    return np.argsort(sample)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 10\n",
    "hidden_size = 35\n",
    "output_size = len(char.keys())\n",
    "n_layers = 2\n",
    "batch_size = 16\n",
    "GAMMA = 0.99 # for exponential moving avarage constant from paper\n",
    "\n",
    "\n",
    "token_num = len(token.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, predict_len=100, variance=0.01):\n",
    "    input_token = \"START\"\n",
    "    input_token = token_to_tensor(input_token)\n",
    "    model.init_hidden_normal(variance=0.5)\n",
    "    prediction = [\"\"] * batch_size\n",
    "    program_probs = np.ones((1, batch_size))\n",
    "    \n",
    "    batched_input = torch.zeros((batch_size, token_num)).long()\n",
    "    batched_input = batched_input + input_token\n",
    "    batched_input = Variable(batched_input.view(token_num, batch_size))\n",
    "    \n",
    "    for i in range(predict_len):\n",
    "        output_probs = model.forward(batched_input)\n",
    "        top_probs, next_tokens = torch.max(output_probs, 1)\n",
    "        next_tokens = next_tokens.data.numpy()\n",
    "            \n",
    "        batched_input = torch.zeros((batch_size, token_num)).long()\n",
    "        batched_input[np.arange(batch_size), next_tokens] = 1\n",
    "        batched_input = Variable(batched_input.view(token_num, batch_size))\n",
    "        \n",
    "        program_probs *= top_probs.data.numpy()\n",
    "        \n",
    "        #prediction = map(\"\".join,zip(prediction, char[]))\n",
    "        for i in xrange(batch_size):\n",
    "            prediction[i] += char[next_tokens[i]]\n",
    "        \n",
    "    return prediction, program_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BFgen(token_num, embedding_size, hidden_size, output_size, n_layers, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "progs, probs = evaluate(model, 5, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[']]]]]', ']]]]]', ']]]]]', ']]]]]', ']]]]]', ']]]]]', ']]]]]', ']]]]]', ']]]]]', ']]]]]', ']]]]]', ']]]]]', ']]]]]', ']]]]]', ']]]]]', ']]]]]']\n"
     ]
    }
   ],
   "source": [
    "print(progs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_PG(model, reward_f, predict_len = 100, N = 1000):\n",
    "    objective = Variable(torch.FloatTensor([1]))\n",
    "    baseline = 0\n",
    "    for i in range(0, N, model.batch_size):\n",
    "        input_token = \"START\"\n",
    "        input_token = token_to_tensor(input_token)\n",
    "        model.init_hidden_normal(variance=0.5)\n",
    "        prediction = [\"\"] * model.batch_size\n",
    "        policy_probs = Variable(torch.zeros(model.batch_size).float())\n",
    "\n",
    "        batched_input = torch.zeros((batch_size, token_num)).long()\n",
    "        batched_input = batched_input + input_token\n",
    "        batched_input = Variable(batched_input.view(token_num, model.batch_size))\n",
    "\n",
    "        for i in range(predict_len):\n",
    "            output_probs = model.forward(batched_input)\n",
    "            top_probs, next_tokens = torch.max(output_probs, 1)\n",
    "            next_tokens = next_tokens.data.numpy()\n",
    "            \n",
    "            batched_input = torch.zeros((batch_size, token_num)).long()\n",
    "            batched_input[np.arange(batch_size), next_tokens] = 1\n",
    "            batched_input = Variable(batched_input.view(token_num, model.batch_size))\n",
    "            \n",
    "            # accumulate objective\n",
    "            torch.log(top_probs)\n",
    "            policy_probs += top_probs\n",
    "            \n",
    "            # save chars\n",
    "            for i in xrange(batch_size):\n",
    "                prediction[i] += char[next_tokens[i]]\n",
    "        \n",
    "        # calculate rewards\n",
    "        rewards = map(reward_f, prediction)\n",
    "        # exponential moving avarage\n",
    "        print(rewards)\n",
    "        for i, reward in enumerate(rewards):\n",
    "            baseline = reward * GAMMA + (1 - GAMMA) * baseline\n",
    "            rewards[i] = reward - baseline\n",
    "            \n",
    "        rewards = Variable(torch.FloatTensor(rewards))\n",
    "        objective += (rewards * policy_probs).sum()\n",
    "    return objective / N\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0.010000000000000009, 9.999999999998899e-05, 1.0000000000287557e-06, 1.0000000050247593e-08, 1.000000082740371e-10, 9.999778782798785e-13, 9.992007221626409e-15, 1.1102230246251565e-16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.1477\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(objective_PG(model, lambda x : 1, N=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
